\contentsline {figure}{\numberline {1}{\ignorespaces This figure shows a setting in which a deterministic detector underperforms a stochastic detector. Blue dots correspond to benign samples. Red dot is a malicious sample. Dashed semi-circle is bounds the region of possible obfuscation of the malicious sample. In Fig. \ref {fig:toy-example-deterministic-detector} the blue line is a decision line of a deterministic detector. The arrow shows the obfuscation path from a primary sample to the obfuscated one. Fig. \ref {fig:toy-example-stochastic-detector} depicts contours of a stochastic detector where blue-shaded lines outline areas of high benign-ness probability and red-shaded lines conversely high maliciousness probability. The shadow line is a $50 \%$ boundary. The stochastic detector gives more optimal solution to the detector's optimisation problem (Prop. \ref {prop:detector_optimisation}) \relax }}{23}{figure.caption.7}
\contentsline {figure}{\numberline {2}{\ignorespaces The histograms show distributions of activity captured in March, 2019 among users of a URL reputation service, located in the Czech Republic. The dataset is provided by Trend Micro Ltd. Fig. \ref {fig:request-time-histogram} depicts request day-time distribution, Fig. \ref {fig:requests-per-activity-histogram} shows the amount of requests that is sent in one day activity of a user. Fig. \ref {fig:score-histogram} shows the distribution of a URL reputation score which is associated with a URL query. Finally, Fig. \ref {fig:url-frequency-histogram} shows the repetitive nature of such a reputation service.\relax }}{47}{figure.caption.21}
\contentsline {figure}{\numberline {3}{\ignorespaces Successful attack rate (SAR) as a function of the false positive rate (FPR). Note that, as the FPR threshold is increased, both detectors become more robust. At all FPR levels and with both attack types, the adversarial detector outperforms the anomaly detector.\relax }}{51}{figure.caption.35}
\contentsline {figure}{\numberline {4}{\ignorespaces Primary Goals \IeC {\textendash } \textsf {No-Activity} Dependency. We found that there is an emerging pattern in the obfuscation ability against the adversarial detector. Some primary goals tend to be too costly to be obfuscated so the algorithm turns them to \textsf {No-Activity} . This figure shows the pattern: we draw primary goals (primary URL sets) that are modified to \textsf {No-Activity} in red and primary goals that are turned into an activity history in blue. The axes correspond to parameters of the primary goals we generated: the x-axis shows a number of URLs with a malicious reputation score in a primary goal $ U^{\mathsf {pr}} $; the y-axis shows the number of unrated URLs. The figure depicts individual primary goals as dots and an estimated density distributions with contours. Clearly, primary goals with more $10$ malicious URLs tend to become \textsf {No-Activity} whereas primary goals with fewer than $10$ malicious URLs tend to be conversed to an obfuscated activity history and a corresponding attack is carried out. All data are from a test set of primary goals and the results are taken from an attack against the adversarial detector with $FPR = 0.1\%$.\relax }}{53}{figure.caption.38}
\contentsline {figure}{\numberline {5}{\ignorespaces The figures plot single activity histories as points in a feature space. All images are PCA-transformations of the feature space with identical principal components. Black points are \textsf {No-Activity} , red points are obfuscation activity of individual attacker instances, blue points are benign activity histories. The contours correspond to the class posteriori probability modelled with the detector. All pictures show the results of the adversarial detector. Upper left, the results of the gradient attack with $400$ iterations \IeC {\textendash } NAR is $57.33\%$. Upper right, attack with $800$ iterations - NAR is $29.33\%$. Bottom left, attack with $800$ iterations but less expensive per-request cost, $L_u = 0.005$. Bottom right, attack with $1600$ iterations but extremely inexpensive per-request cost, $L_u = 0.0005$. Note that attacks are located in different areas as we change attacker's costs. In the extreme case (bottom right), nearly all attacks are located beyond the detector's contours.\relax }}{54}{figure.caption.40}
\contentsline {figure}{\numberline {6}{\ignorespaces The figures plot single activity histories as points in a feature space. All images are PCA-transformations of the feature space with identical principal components. Black points are \textsf {No-Activity} , red points are obfuscation activity of individual attacker instances, blue points are benign activity histories. The contours correspond to the class posteriori probability modelled with the detector. The pictures compare the shape of detector's posterior class probability that is depicted wtih contours. On left, the adversarial detector shapes its contours to reflect possible attacks, whereas, on right, the anomaly detector omits the attacker's model and thus adjusts its contours to face all possible anomalies. This gives the adversarial detector advantage in that it reflect the attck distribution.\relax }}{58}{figure.caption.45}
