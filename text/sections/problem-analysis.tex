\section{Problem Analysis}

\todo{What is a proper name for this section?}

In the present state of Internet, it is common for a site owner to run
models classifying users or their behaviour. The task spans from user’s
interests specification to detecting deviating activity. Since such
applications are becoming more popular, one may expect the users to
modify their behaviours once they know they are being tracked and
classified. Moreover, behaviour modification may very well be of
rational nature, especially when a malicious user exploits loopholes or
carries out lawless activity in order to pursuit its goal.

In other words, if there is a cost for being disclosed or seen as a
certain category, the users will examine their actions to optimise for
lower cost. As a result, machine learning models of any kind aiming to
capture behaviours of those users necessarily need to have the adversary
nature incorporated in their design.

The straight-forward approach of solving this task would be to collect
many examples of both kinds of user activity; that is to asses a dataset
containing well-represented both malicious and benign users. This
approach would follow the standard ERM framework and would give an
activity classifier that minimises expected risk but omits the
adversarial nature. However, one might arrive at difficulties during the
construction of a balanced dataset for there is usually very few records
of malicious activity, disproportionally less than the collection of
normal, benign users. Also, and more importantly, the malicious actors
modify their attack vectors once their method is exposed or they
discover details concerning the detector.

Taking that into account, we consider the setting as a game of a classifier
competing with a body of malicious users. This approach necessarily
modifies the ERM framework and enhances it with game-theoretic notions.

This section first discusses the use-case which motivates this work.
Then, a suitable threat model is prosed and the game is formally
defined, supplemented with reasoning for given choices.

\todo{Emphasise the general adversarial machine learning problem as the core of this section.}
f
\todo{revisit the section introduction to inform about all subsections and refer to them}

A malicious activity detection system is essentially a classifier that classifies users based on their behaviour. This in principle is a machine learning problem of finding a classifier $f \in \F$ minimising expectation of detection loss $\ell_{-1}$. The detector is a mapping $f: \X \mapsto \C$ which takes vectors $x \ in \X$ on its input and produces a decision $d \in \C$. However, the ground variable representing a discrete input object is a user's activity history $h \in \H$ which is translated to a corresponding feature vector with a feature map $\Phi: \H \mapsto \X$.

All variables and functions related strictly to a detector are subscripted with $-1$, whereas we use $+1$ in the attacker's case. This choice follows Brückner et al \cite{stackelberg_games}.

As already mentioned, minimising the expected risk – as it is done in general classification problems – does not help to solve the problem of detection. The expected risk minimisation framework (ERM) consists of identifying an optimal classifier $f$ that minimises expectation of $\ell_\minus : \C \times \C \mapsto \R$ over the set $\X \times \C$.

 The expected risk can be formulated as a convex combination of risks conditioned on a class.Assuming there is two classes, i.e. $\C = \{ \ben, \mal \}$, the expected risk can be rewritten as a combination of the risk attained on the malicious class and the risk attained on the benign class.

\begin{definition}\label{def:risk}
    Let the risk attained on the malicious a class $\mal$, $R_\minus(f \mid \mal)$, be the expectation of the loss conditioned on class $\mal$. Let the risk attained on the benign class $\ben$, $R_\minus(f \mid \ben)$, be the expectation of the loss conditioned on a class $\ben$.

    \begin{align}
        R_\minus(f \mid \mal)  &= \E_{x} \left \ell_\minus(f(x), \mal) \mid \mal \right \\
        R_\minus(f \mid \ben)  &= \E_{x} \left \ell_\minus(f(x), \mal) \mid \ben \right
    \end{align}

\end{definition}

\begin{definition}[Detector's Expected Risk Minimisation]\label{def:erm}
    In standard classification, the optimal classifier $f^*$ is the solution of the following problem:

    \begin{equation}\label{eq:erm}
        \begin{aligned}
        & \underset{f \in \F} {\text{minimise}}
        & & p(\ben) \cdot R_\minus(f \mid \ben)
        +
        p(\mal) \cdot R_\minus(f \mid \mal)
        \end{aligned}
    \end{equation}
\end{definition}


\subsection{Specifics of Adversarial Machine Learning}

In this section, we examine adversarial machine learning in the domain of network security in general terms. The central task is to detect malicious users in the network without ideally affecting legitimate users.

The expectations in Def. \ref{def:erm} are usually estimated from a set of examples of each class. However, in the detection problem there are not enough examples of malicious behaviour and, in addition, this behaviour changes reflecting the current detector. This imposes two critical properties of adversarial machine learning:

\begin{itemize}
\item
    The priori class probabilities are not known.
\item
    An individual attacker follows its private objective and (possibly rationally) chooses actions minimising its cost.
\end{itemize}

\subsection{Property 1: Unknown Class Probabilities}

To reflect the first property, we redefine the detection problem to comply with the Neyman-Pearson Task \ref{sec:neyman-pearson}. Since we aim to detect malicious activity, the false positives comprise benign users classified as malicious. And, vice-versa, the false negatives are malicious users classified as benign.

\begin{definition}[Neyman-Pearson Task]\label{def:np_task}
    The Neyman-Pearson Task translates to minimising the expected loss conditioned on the malicious class while the expected loss conditioned on the benign class is maintained lower than a threshold.

    \begin{equation}\label{eq:erm}
        \begin{aligned}
        & \underset{f \in \F} {\text{minimise}}
        & & R_\minus(f \mid \mal) \\
        & \text{subject to}
        & & R_\minus(f \mid \ben) \leq \tau_0 \\
        \end{aligned}
    \end{equation}
\end{definition}

Using this formulation, prior class probabilities are omitted and, in addition, the task reflects the nature of security detection problems in which there is a hard constraint on false positives. In other words, we aim to find a classifier $f$ that does not affect legitimate activity but given this constraint is the best detector of malicious activity. As shown later, this form is particularly useful if the detector is a neural network.

\subsection{Property 2: Adversarial Setting}
By assuming an attacker is a rational actor that pursuits its goal, the setting of statistical learning changes to an adversarial game of two players: a detector and an attacker.

We sort sampled activity into two classes: benign ($\ben$) and malicious ($\mal$). The former is activity generated by legitimate users and the later is activity produced solely by attackers in pursuit of their objectives.

To avoid detection, each individual attacker obfuscates its primary activity by acting nearly as a legitimate user. However, if a good detector is deployed the obfuscation requires a large quantity of legitimate activity. If the obfuscation effort is too costly the attacker may seize to attack at all.

The obfuscated activity of an attacker, in consequence, is recorder by the detector and stored as an activity history based on which the detector assigns a label to it. The intuitive goal of the detector is to label legitimate activity history as benign, i.e. $\ben$, and the activity history generated by an attacker as malicious, i.e. $\mal$.

In the following sections, we closely explore and examine the motivations and aspects implied by the adversarial setting in network security.

\subsubsection{Stackelberg Game}
In practice, the detector is fixed after deployment and the choice of its particular form and parameters necessarily occurs before the deployment. This is a case of a Stackelberg game \cite{stackelberg_games} in which the detector is a leader and the attacker is a follower. For the sake of simplicity we assume the Strong Stackelberg Equilibrium is played.

In a Stackelberg game, the follower plays a best response to the leader's public strategy and the leader optimises this strategy accounting the follower's best response. In such a setting, the leader optimally plays a mixed strategy.

This means, the attacker obfuscates its activity optimally without a need of randomisation by playing a best response to detector's strategy.

As mentioned, the detector may necessarily randomise its actions to achieve the optimal cost. This translates to a detector playing a mixed strategy $\sigma(f) : \F \mapsto [ 0,1 ] $ instead of a single particular $f$.

\begin{definition}\label{def:stochastic-detector}
    A stochastic detector $D: \X \mapsto \C$ is a probability distribution $p(d \mid x )$ generating a decision $d \in \C $ conditioned on an observed sample $x \in \X$. $D_\sigma$ is given by a detector playing a mixed strategy $\sigma: \F \mapsto [0,1]$:

    \begin{equation}
        D_\sigma(d|x) = \sum_{f: f(x) = d} \sigma(f)
    \end{equation}

\end{definition}

\subsubsection{Information Available to Attacker}
We assume the attacker has full knowledge of the detector's structure and parameters.

\subsubsection{Attacker}
We model the attacker as a rational actor which plays the action minimising its expected costs. The particular form of costs and actions depends largely on the domain. Therefore, here we only present general notions defining the attacker and, later in Section \ref{sec:attacker}, we propose the attacker's model that suits the running example of attacks to a URL reputation service.

We propose all attackers follow the same objective and they differ only in their particular primary goal. That is, the activity obfuscation is practically the same task shared by all attackers and two attackers differ in what they aim to obfuscate.

For that reason, the model of an attacker considers a common body of attacker instances in which an individual attacker instance is thoroughly defined by its primary goal $g \in \G$. The common shared obfuscation function $\psi: \G \mapsto \H$ takes a primary goal $g$ on its input and maps it to an activity history $h = \psi(g)$ that obfuscates the primary goal.

\todo{obfs function maps to S(g), maybe make definition of psi}

\begin{definition}
    The attacker's risk $R_\plus : \Psi \times \F \mapsto \R$ is given as the expectation of its loss $\ell_\plus : \G \times \Psi \times \C \mapsto \R$. That is:

    \begin{equation}
        R_\plus(\psi, f) = \E_g \left \ell_\plus (g, \psi, f(\Phi(\psi(g)))) \right
    \end{equation}
\end{definition}

\todo{couple of words what the loss comprises.}

Relating to game theory, the obfuscation function $\psi$ is an attacker's action and its best response to $\sigma$ is given by minimising the attacker's expected risk $\E_{f \sim \sigma} R_\plus(\psi, f)$.

\begin{proposition}[Attacker's best response]\label{prop:br}
    The attacker's best response $\BR(\sigma)$ to a mixed strategy $\sigma$ is a set of obfuscation functions $\psi: \G \mapsto \H$ that are the minimisers of the expectation of the attacker's loss $\ell_\plus$.

    \begin{equation}
        \BR(\sigma) = \argmin_\psi \E_{g,d} \left \ell_\plus (g, \psi, d) \right
    \end{equation}
\end{proposition}

\begin{proof}
    \begin{align}
        \BR(\sigma) &= \argmin_\psi \E_{f \sim \sigma} R_\plus(\psi, f) \\
            &= \argmin_\psi \E_{f, g} \left \ell_\plus (g, \psi, f(\Phi(\psi(g)))) \right \\
            &= \argmin_\psi \sum_f \sum_g  \ell_\plus (g, \psi, f \circ \Phi \circ \psi(g))  \cdot  p(g) \cdot  \sigma(f)  \\
            &= \argmin_\psi \sum_g \sum_d \sum_{f: f \circ \Phi \circ \psi(g) = d}  \ell_\plus (g, \psi, d) \cdot  p(g)\sigma(f) \\
            &= \argmin_\psi \sum_g \sum_d \ell_\plus (g, \psi, d)  \cdot   p(g)  \cdot  \sum_{f: f \circ \Phi \circ \psi(g) = d} \sigma(f) \\
            &= \argmin_\psi \sum_g \sum_d \ell_\plus (g, \psi, d) \cdot  p(g)  \cdot D_\sigma(d \mid \Phi \circ \psi(g) ) \\
            &= \argmin_\psi \E_{g,d} \left \ell_\plus (g, \psi, d) \right
    \end{align}
\end{proof}

\subsubsection{Stochastic Detector}
The detector's pure strategy consists of a particular detector $f$. However, as proposed above, its optimal strategy is generally mixed and the detector, therefore, randomises its final decision $d \in \C$.

A mixed strategy in case of the detector is a probability distribution $\sigma : \F \mapsto [0,1]$ which assigns a probability to each particular detector $f$. The decision $d$ representing the estimated class of a sample $x$ is, consequently, a random variable whose probability distribution is the aggregate of probabilities $\sigma(f)$ for which $f(x) = d$. To capture that, we defined a decision distribution $D(d|x)$ in Def. \ref{def:stochastic-detector}.

In this work, we model $D(d|x)$ with a neural network which fruitfully allow us to bypass potentially infinite enumeration of detectors from $\F$. The detector's mixed strategy is, in conclusion, represented by the distribution $D_\theta (d|x)$ where $\theta$ is a parameters vector.

As proposed by Brückner et al. \cite{stackelberg_games}, the attacker's impact on the setting can be modelled by a distribution shift. However, in contrast to \cite{stackelberg_games}, in this work we assume only the malicious class activity is governed by adversarial objectives and benign activity is maintained unchanged irrespective to the detector's presence. Taking that into account, we define that the distribution of samples produced by attackers $p(x \mid \mal)$ is shifted in reaction to the presence of a deployed detector $D$ and changes to $\dot{p}(x \mid \mal, D)$.

In a standard classification problem, we find $f$ minimising the expected risk. In our adversarial setting, the detector is necessarily a distribution $D$ that is a solution to the Neyman-Pearson Task with a non-stationary distribution of samples $\dot{p}(x \mid \mal)$.

\begin{proposition}
    Let the attacker play a best response $\BR(\sigma)$ to a mixed strategy $\sigma$, then the detector's risk of a mixed strategy $\sigma$ attained on malicious activity, $R_\minus(\sigma \mid \mal)$, is given by the best-case expectation of its loss attained on malicious activity.

    \begin{align}
        R_\minus(\sigma \mid \mal) = \E_f \left R_\minus(f) \mid \mal \right = \min_{\psi \in \BR(\sigma) } \,
            \E_{q, d} \left \ell_\minus (d, \mal) \mid \mal \right
    \end{align}

    Similarly, the detector's risk of mixed strategy $\sigma$ attained on benign activity, $R_\minus(\sigma \mid \ben)$, is given by the expectation of its loss attained on benign activity.

    \begin{align}
        R_\minus(\sigma \mid \ben) = \E_f \left R_\minus(f) \mid \ben \right =
            \E_{h, d} \left \ell_\minus (d, \ben) \mid \ben \right
    \end{align}

\end{proposition}

\begin{proof}

For the risk of a mixed strategy attained on malicious activity, it holds that:
\begin{align}
    R_\minus(\sigma \mid \mal) &= \E_f \left R_\minus(f) \mid \mal \right \\
    &= \E_{f, x} \left \ell_\minus(f(x), \mal) \mid \mal \right \\
    &= \sum_f \sum_x \ell_\minus(f(x), \mal)  \cdot  \dot{p}(x \mid \mal)  \cdot  \sigma(f)
\end{align}

Consider a sample $x$ is generated solely by the attacker (due to the $\mal$ class in the conditional probability). We substitute $x$ for $\Phi \circ \psi (g)$. Assuming a feature map $\Phi: \H \mapsto \X$ projects each $h$ to one particular feature vector $x$ and a malicious activity history $h$ is  given by a primary goal $g$ obfuscated by a best response obfuscation function $\psi \in \BR(\sigma)$, the sum of probabilities $p(g)$ for which $\Phi \circ \psi(g) = x$ gives the non-stationary probability $\dot{p} (x \mid \mal)$.

\begin{align}
    \dot{p} (x \mid \mal) &= \sum_{h: \Phi(h) = x} \dot{p}(h \mid \mal) \\
    &= \sum_{h: \Phi(h) = x} \sum_{g: \psi(g) = h} p(g) \\
    &= \sum_{g: \Phi \circ \psi(g) = x} p(g)
\end{align}

Using the substitution and considering the best-case, we arrive at:

\begin{align}
    R_\minus(\sigma \mid \mal) &= \sum_f \sum_x \ell_\minus(f(x), \mal)  \cdot  \dot{p}(x \mid \mal)  \cdot  \sigma(f) \\
    &= \min_{\psi \in \BR(\sigma)}
        \sum_f \sum_g \ell_\minus(f \circ \Phi \circ \psi(g), \mal)  \cdot p(g)  \cdot \sigma(f) \\
    &= \min_{\psi \in \BR(\sigma)}
        \sum_g \sum_d \ell_\minus(d, \mal) \cdot p(g)  \cdot \sum_{f: f \circ \Phi \circ \psi(g) = d} \cdot \sigma(f) \\
    &= \min_{\psi \in \BR(\sigma)}
        \sum_g \sum_d \ell_\minus(d, \mal) \cdot p(g) \cdot D_\sigma(d \mid \Phi \circ \psi(g)) \\
    &= \min_{\psi \in \BR(\sigma) } \,
        \E_{q, d} \left \ell_\minus (d, \mal) \mid \mal \right
\end{align}

Similarly for the risk of a mixed strategy attained on benign activity:

\begin{align}
    R_\minus(\sigma \mid \ben) &= \E_f \left R_\minus(f) \mid \ben \right \\
    &= \E_{f, x} \left \ell_\minus(f(x), \ben) \mid \ben \right \\
    &= \sum_f \sum_x \ell_\minus(f(x), \ben)  \cdot p(x \mid \ben) \sigma(f) \\
    &= \sum_f \sum_h \ell_\minus( f \circ \Phi(h) ), \ben) \cdot  p(h \mid \ben)  \cdot \sigma(f) \\
    &= \sum_h \sum_d \ell_\minus(d, \ben)  \cdot p(h \mid \ben) \cdot  D_\sigma(d \mid \Phi(h)) \\
    &= \E_{h, d} \left \ell_\minus(d, \ben) \mid \ben \right
\end{align}
\end{proof}

\begin{definition}
    For simplicity, we interchangeably use $\sigma$ and $D_\sigma$ and $D_\theta$ as the detector's strategy. Thus:

    \begin{equation}
         R_\minus(D_\sigma \mid \cdot) = R_\minus(\sigma \mid \cdot) = R_\minus(\theta \mid \cdot)
    \end{equation}

\end{definition}


\begin{proposition}[Detector's optimisation problem]
    Let the detector minimise the expected risk attained on malicious activity, while maintaining the expected risk attained on benign activity upper-bounded by $\tau_0$. Let the attacker minimise its expected risk. Then the stochastic detector $D_\theta$ parametrised by $\theta$ and the obfuscation function $\psi$ which are the solution to the following bi-level optimisation problem are the Stackelberg equilibrium.

    \begin{equation}\label{eq:detector_optimisation}
        \begin{aligned}
        & \underset{\theta, \psi} {\text{minimize}}
        & & \E_{q, d} \left \ell_\minus (d, \mal) \mid \mal \right \\
        & \text{subject to}
        & & \E_{h, d} \left \ell_\minus(d, \ben) \mid \ben \right \le \tau_0 \\
        & & & \psi \in \argmin_{\psi'} \E_{g,d} \left \ell_\plus (g, \psi', d) \right
        \end{aligned}
    \end{equation}

\end{proposition}

\begin{proof}
 The proposition follows directly from the definitions and propositions above.
\end{proof}

\todo{This is cool as it follows directly from ERM when three assumptions are added: Neyman-Pearson, Rational Attacker, Stackelberg Game}

\todo{Smooth out the sequence of those propositions by mix-in explanations. This is a key train thought and it is important to stress it out clearly.}


\subsection{Assumption on Losses}
As it is common in ERM, we expect the detector's loss $\ell_\minus$ is a zero-one loss. This simplifies the primary objective in the detector's optimisation problem.

\begin{proposition}\label{prop:ben_loss}
    Let the detector's loss $\ell_\minus$ be a zero-one loss. Then the detector's risk attained on malicious activity $R_\minus(\theta \mid \mal)$ is the expectation of the posteriori probability of a benign class conditioned on malicious activity. Similarly for the risk attained on benign activity $R_\minus(\theta \mid \mal)$:

    \begin{align}
        R_\minus(\theta \mid \mal) &= \min_{\psi \in \BR(\sigma)} \E_g \left D_\theta(\ben \mid \Phi \circ \psi (g)) \mid \mal \right \\
        R_\minus(\theta \mid \ben) &= \E_h \left D_\theta(\mal \mid \Phi (h)) \mid \ben \right
    \end{align}
\end{proposition}

\begin{proof}
    The proof is straight-forward.

    \begin{align}
        R_\minus(\theta \mid \mal) &= \min_{\psi \in \BR(\sigma) } \,
            \E_{q, d} \left \ell_\minus (d, \mal) \mid \mal \right \\
           &= \min_{\psi \in \BR(\sigma) } \,
               \E_q \left \sum_d \ell_\minus (d, \mal) D_\theta(d \mid \Phi \circ \psi(q)) \mid \mal \right \\
           &= \min_{\psi \in \BR(\sigma) } \,
               \E_q \left D_\theta(\ben \mid \Phi \circ \psi(q)) \mid \mal \right \\
        R_\minus(\theta \mid \ben) &= \E_{h, d} \left \ell_\minus (d, \ben) \mid \ben \right \\
           &= \E_h \left \sum_d \ell_\minus (d, \ben) D_\theta(d \mid \Phi (h)) \mid \ben \right \\
           &= \E_h \left D_\theta(\mal \mid \Phi (h)) \mid \ben \right
    \end{align}

\end{proof}

The posteriori probability of the stochastic detector $D_\theta(d \mid x)$ is explicitly modelled by neural network in this work. Thus we prefer the risk explicitly contains the term. However, this does not hold generally and in some cases it is more fruitful to estimate the risk as expectation of loss values (e.g. reinforcement learning).

The same trick which was used in case of the defender cannot by applied to the attacker. The attacker's loss $\ell_\plus: \G \times \Psi \times \C \mapsto \R$ is more complex. Naturally, it consists of two components: a public and a private term. The public cost reflects the adversarial objective of escaping detection (e.g. detection probability). The private cost penalises the attacker for too costly obfuscation and is not necessarily adversarial to the detector's cost.

This also shows the game is a non-zero sum game as the private term in the attacker's loss does not have an adversarial equivalent in the detector's loss.

Following Brückner et al. \cite{stackelberg_games}, we defined the attacker as a shared body of attacker instances. However, if the attacker's loss is defined conveniently, the attacker's optimisation problem decomposes and it can be solved independently for each attacker's instance. The convenient form of the loss is shown Tab. \ref{tab:attacker_loss}.

\begin{table}
    \centering

        \begin{tabular}{|l|l|}
            \hline
            $d$    & $\ell_\plus(g, \psi, d)$         \\ \hline
            $\ben$ & $\Omega_\plus(g, \psi(g))$       \\ \hline
            $\mal$ & $L_0 + \Omega_\plus(g, \psi(g))$ \\ \hline
        \end{tabular}

    \caption{Table to test captions and labels}
    \label{tab:attacker_loss}
\end{table}

The motivation of this particular form of the loss is simple. If an attacker is detected it pays the amount $L_0$ for acquiring a new license or an account so that it is able to carry out further activity. However, the more complex activity histories it creates to obfuscate its primary goal, the more costly carrying out such activity is. This is represented by $\Omega_\plus: \G \times \H \mapsto \R$.

Recall that the obfuscation function $\psi(g)$ is constrained by the set of activity histories $S(g)$ such that $\psi(g) \in S(g)$, i.e. $\psi(g)$ can only create activity histories in $S(g)$. The set $S(g)$ in practice defines activity histories that the attacker is able to construct from $g$.

\begin{proposition}\label{prop:mal_loss}
    Let the attacker's loss be defined by Tab. \ref{tab:attacker_loss}. Let the attacker's private cost be a function $\Omega_\plus: \G \times \H \mapsto \R$. Then the attacker's best response problem of finding the optimal obfuscation function $\psi^*$ decomposes to identifying $\Psi^*(g)$ such that $\psi^*(g) \in  \Psi^*(g) \subset S(g) $ and $\Psi^*(g)$ is the set of solution to the following problem.

    \begin{equation}
        \Psi^*(g) = \argmin_{h \in S(g)} L_0 \cdot D_\sigma(\mal \mid \Phi(h)) + \Omega_\plus(g, h)
    \end{equation}

\end{proposition}

\begin{proof}
    Proposition \ref{prop:br} defines the attacker's best response problem in which the expectation is over variables $g$ and $d$.

    \begin{align}
        \psi^* \in \BR(\sigma) &= \argmin_\psi \E_{g,d} \left \ell_\plus (g, \psi, d) \right \\
        &= \argmin_\psi \E_g \E_d \left \ell_\plus (g, \psi, d) \right \label{eq:inner_exp}
    \end{align}

    Let us substitute the loss $\ell_\plus$ for its tabular form in Tab. \ref{tab:attacker_loss}. The inner expectation in Eq. \ref{eq:inner_exp} simplifies and becomes:

    \begin{align*}
        \E_d \left \ell_\plus (g, \psi, d) \right & = \Omega_\plus (g, \psi(g) ) \cdot D_\sigma(\ben \mid \Phi \circ \psi (g)) \, + \\
        & \quad + (L_0 + \Omega_\plus(g, \psi(g))) \cdot D_\sigma(\mal \mid \Phi \circ \psi (g)) \\
        & = \Omega_\plus(g, \psi(g)) \cdot ( 1 - D_\sigma(\mal \mid \Phi \circ \psi (g)) ) \, + \\
        & \quad + (L_0 + \Omega_\plus(g, \psi(g))) \cdot D_\sigma(\mal \mid \Phi \circ \psi (g)) \\
        & = L_0 \cdot D_\sigma(\mal \mid \Phi \circ \psi (g)) + \Omega_\plus(g, \psi(g))
    \end{align*}

    This gives us a simplified of the best response problem:

    \begin{equation*}
        \argmin_\psi \E_g \left L_0 \cdot D_\sigma(\mal \mid \Phi \circ \psi (g)) + \Omega_\plus(g, \psi(g)) \right
    \end{equation*}

    The best response problem now contains only $\psi(g)$ and the expectation can be decomposed. The criterion is minimised if we set $\psi(g)$ to $h$ that minimises $L_0 \cdot D_\sigma(\mal \mid \Phi (h)) + \Omega_\plus(g, h)$. However, the obfuscation function $\psi$ is constrained by $S(g)$. Taking $S(g)$ into account, we arrive at the following form.

    \begin{equation*}
        \psi^*(g) \in \Psi^*(g) = \argmin_{h \in S(g)} L_0 \cdot D_\sigma(\mal \mid \Phi (h) ) + \Omega_\plus(g, h)
    \end{equation*}

    $\Psi^*(g)$ simply denotes the solution of the optimisation problem.

\end{proof}

Having

\begin{proposition}
    Considering the losses in Propositions \ref{prop:ben_loss} and \ref{prop:mal_loss}, the detector's optimisation problems \ref{eq:detector_optimisation} becomes:

    \begin{equation*}
        \begin{aligned}
        & \underset{\theta} {\text{maximise}}
        & & \E_g \left \max_{\adv{h} \in \Psi^*(g)} D_\theta(\mal \mid \Phi (\adv{h})) \mid \mal \right \\
        & \text{subject to}
        & & \E_{h} \left D_\theta(\mal \mid \Phi (h)) \mid \ben \right \le \tau_0 \\
        & & & \Psi^*(g) = \argmin_{h' \in S(g)} L_0 \cdot D_\sigma(\mal \mid \Psi(h')) + \Omega_\plus(g, h')
        \end{aligned}
    \end{equation*}
\end{proposition}

\begin{proof}
    \todo{todo}
\end{proof}

\subsection{Anomaly Detection}
Note the aforementioned problem is related to (unsupervised) anomaly detection in which is is assumed no information about the malicious class is known. Thus, formulating the problem as anomaly detection, we aim to identify a detector for which the expected loss conditioned on benign class is lower than a threshold. The anomaly detection view of the problem is utilised for example if a detector consists of a k nearest neighbours distance estimator.

For our purposes we define anomaly detection as ...

\begin{definition}[Anomaly Detection]\label{def:anomaly_detection}
    Let the optimal anomaly detector be a distribution $D_\theta(d | x)$, solely parametrised by a vector $\theta$, if it is a solution to the following problem:

    \begin{equation}
        \begin{aligned}
        & \text{find}
        & & \theta \\
        & \text{such that}
        & & \E_h \left D_\theta(\mal \mid \Phi(h)) \mid \ben \right \leq \tau_0 \\
        \end{aligned}
    \end{equation}
\end{definition}
