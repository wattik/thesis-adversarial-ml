\section{Related Work}\label{sec:related_work}

Examining adversarial aspects of various machine learning problems has
currently been a popular topic. Mainly, this was triggered by Goodfellow et al.
\cite{adversarial_examples} who showed that neural networks are susceptible to
adversarial examples. Since then many endeavours have been carried out
to enhance neural networks or other machine learning algorithms by making them robust. Some tried to develop
a provably robust classifier \cite{provable_defenses}, while others reframed the
classification problem to incorporate aspects of game theory
\cite{stackelberg_games}. Huang et al. \cite{huang} identify that there are several assumptions in the machine learning scheme that are often violated. For instance, they consider the data distribution is non-stationary. Barreno et al. \cite{barreno} point out that some data may be generated by adversaries who play an instance of a deception game - that is they purposely adjust their actions to cover their true intention.

Despite most of the related work deals with image
classification, efforts to utilise the same notions in computer security
have been seen too \cite{adversarial_malware_pe, adversarial_malware_binary, adversarial_malware}. Susceptibility to adversarial
examples is, however, not the only weakness adversaries exploit, they
also are able to modify future training datasets in their favour
\cite{antidote}.


\subsection{Adversarial Machine Learning}

Lowd et al. \cite{good_word_attacks} explore obfuscate strategies yielding spams that
circumvent a spam filter. The authors consider attacks which are based
on adding words to a spammy e-mail, while other modifications are not
allowed. Three pools of words are defined: in the first attack
random words from a dictionary are drawn; the second attack utilises
common legitimate e-mail words;~and in the third attack, words that
are likely to appear in legitimate e-mails but are uncommon in spams are
added.

To select the final set of words with the greatest effect from one of
the three word pools, a black box threat model is used. In particular,
the attacker repeatedly calls the detector to identify words, which make
the detector label the spam as benign. As expected, the last pool of
words mentioned outperforms the others. Moreover, this shows that
additive changes to a malicious object are sufficient for obfuscating
the detector (within this domain). The authors claim they are able to
add words to spams in such a way the tested detection models do not
detect 50\% of them. We similarly design the good queries attack \ref{sec:good_queries_attack} in which we obfuscate user's activity by adding legitimate requests.

To reflect the successful attack algorithm, a defense strategy is
proposed. It is shown that a robust detector which uncovers the adjusted
spammy e-mails can be obtained by simply retraining the model on data
now containing the attacks. However, the authors comment, a repeated
obfuscation with a new set of effective words may again defeat the
detector.

A similar notion is seen in more advanced classification models. For
instance, deep neural networks are a popular class of classifiers
nowadays for their performance in a great range of fields. They were
shown to outperform other methods in image classification (ImageNet
Challenge \cite{image_net}), natural language processing \cite{transformer}
and in many other fields. However, it was found that neural networks
are susceptible to artificially crafted images. In particular, Goodfellow et al. \cite{adversarial_examples} show an adversarial example may be labeled as an
arbitrary class when accordingly adjusted. Moreover, despite the
transformation of an input image is substantially bounded, for example
by $l_\infty$ norm, classifiers based on neural networks are prone to
be circumvented anyway\cite{adversarial_examples_2}. The susceptibility to
adversarial samples follows the same observation in spam filtering – a
good classifier is not necessarily robust to test time data
manipulation.

As soon as it was recognised the neural networks contain built-in
vulnerabilities which are exploitable, endeavours to improve the
architecture were carried out. To address the weakness, some of the
following work focus on a model definition and consider possible attacks
already in the model design. This approach is summarised by Madry et al.
\cite{towards_deep_learning_models} who study adversarial examples
 in image classification.
The authors identify that expected risk minimisation (ERM) does not
necessarily give models robust to adversarially crafted samples.

Their work extends the training framework based on ERM by a threat model
in which each data point $x \in \mathbb{R}^N$ is assigned a set of
perturbations $S(x) \subseteq \mathbb{R}^N$ that is available to the
adversary. The authors work with $S_\epsilon(x)$ that contains
perturbations bounded by $l_\infty$, creating an
$\epsilon$-hyper-cube around each $x$:

\begin{equation}
S_\epsilon(x) = \{x' \in \mathbb{R}^N \, \mid \, l_\infty(x - x') \, \leq \, \epsilon \}
\end{equation}

The norm $l_\infty$ is used for simplicity and roughly represents
human-undetectable image perturbations. Other approaches, however,
consider more complex bounds that capture domain-specific constraints
\cite{adversarial_examples_glasses}.

To fully relate to an adversarial setting, Madry et al. \cite{towards_deep_learning_models} propose that
the adversary maximises the classifier's loss function $L$ by
modifying an image $x$ to an adversarial example
$x' \in S_\epsilon(x)$. This is further incorporated into the ERM
framework, arriving at a saddle point problem:

\begin{equation}
\min_{f \in \F} \, \E_{x, c} \leftb \max_{x' \in S_\epsilon (x)} \,  L(f(x'), \, c) \rightb
\end{equation}

In other words, a solution to the problem gives an optimal robust
classifier $f \in \F$ that is likely to classify all objects
$x \in \mathbb{R}^N$ and their neighbourhood $S_\epsilon (x)$
correctly. We similarly compose a saddle point problem to solve adversarial problems, however, we assume the attacker's goal is general and its utility does not only correspond to classification accuracy.

The saddle point problem given above consists of two sub-problems:
training the neural network and performing the inner maximisation.
\cite{towards_deep_learning_models} approach the training part with Stochastic Gradient
Descent (SGD) as it is commonly done in neural networks, while solving
the inner maximisation task with Projected Gradient Descent (PGD)
\cite{pgd}. They conclude the ERM framework extended by this
specific threat model gives a training method that is able to train
neural networks in the adversarial setting and to produce classifiers
robust to $l_\infty$ bounded image perturbations. In addition, they
find lower error is obtained with higher capacity models, suggesting
that a robust model requires more parameters (eg. layers in neural
networks).

To address the susceptibility to adversaries, several proposals of
neural networks enhancements were submitted at ICLR 2018. However, seven
out of nine were shown to be flawed due to following a similar
ineffective scheme of masking the gradients \cite{obfuscated_gradients}.

In their paper, Athalye et al. \cite{obfuscated_gradients} suggest there are three groups of
gradient masking: first, a non-differentiable layer is inserted between
the network layers; second, a classifier randomises its outputs; and
third, a function transforms the input in such a way backward gradient
explodes or vanishes. Showing that the submitted defensive methods
follow the schemes, the authors succeeded in circumventing 7 of 9
proposed models. Concretely, they replaced or removed defensive
non-differentiable components accordingly to estimate the gradient and
crafted adversarial samples with PGD.

In our problem, the attacker's optimisation criterion is not differentiable because the search space is discrete and hierarchically composed. Inspired by this approach, we solve the problem similarly and parametrise non-differentiable elements of the criterion with an interpolating function. We then solve the attacker's optimisation with PGD.

In addition, Athalye et al. \cite{obfuscated_gradients} suggest that randomisation of the classifier decision does not work for this only extends the iterations needed to acquire true gradient but does not increase robust aspects. We however use a stochastic detector whose decision is a realisation of a modelled posteriori probability – in other words, we too randomise the classifier's output. Importantly, we do so, because we model a mixed strategy which a detector as a player necessarily plays in an equilibrium. To support the claim, we show in Sec. \ref{sec:stochasticity_importance} that a stochastic classifier is more general than a deterministic classifier and outperforms it.

A key aspect of adversarial machine learning is a definition to what extent the attacker knows private parameters of a defender. In a white box approach, the attacker has full access to a gradient, a structure or parameters of the classifier \cite{obfuscated_gradients, provable_defenses, stackelberg_games, barreno}. In a black-box approach, the attacker gains access usually only to the output of the classifier and estimates other private setting from this output \cite{black-box, black_box_adversarial_attacks}. In our game setting, we work with a white box thread model.

\subsection{Provable robustness}

Until now, all presented efforts to improve the neural networks
susceptibility were approached empirically and usually without providing
provable defenses \cite{towards_deep_learning_models}. A method that aims to give provable
resistance to adversarial samples was proposed by Kotler et al. \cite{provable_defenses} who
examine a novel network architecture that provably classifies all
objects in a convex neighbourhood of a given image correctly. To achieve
that, Kotler et al. \cite{provable_defenses} redefine a ReLU \cite{relu} in such a way it
is not a function anymore but rather a set of linear constrains yielding
a convex polytope; i.e. a ReLU $y = \max \{0, x\}$ becomes:

\begin{align}
    y &\geq x \\
    y &\geq 0 \\
    y(l-u) &\leq -ux + ul
\end{align}

where $u$ and $l$ are an upper, respectively lower bound of $x$.
The bounds are unknown and need to be estimated for each ReLU.

With a convex relaxation of ReLU, image classification can be rewritten
as a linear program with all components of the network now being linear.
In the training process, the weights of the relaxed neural network are
optimised so that the network correctly classifies not only the input
image but also its convex embedding. More specifically, using a
$l_\infty$ norm a $\epsilon$-neighbourhood of an input sample is
embedded by a convex polytope and the network learns to disallow any
adversarial samples in it.

Solving the optimisation problem in its LP form with a standard LP
solver is not tractable due to a great number of variables needed to
express state-of-the-art deep neural networks. However, the LP can be
conveniently used to form an upper bound on robust classification
accuracy. Now, this upper bound combined with the ReLU input bounds
estimation becomes fully differentiable. The training process follows
standard SGD \cite{sgd} and gives a robust classifier that allows provably at most
6\% error on MNIST \cite{mnist}. In contrast, a classical neural architecture is
vulnerable up to 80\% error \cite{provable_defenses}.

\subsection{Optimising malware}

In contrast to image classification, the space of inputs is usually
discrete in computer security. An image can be represented as a vector
in $[0, 1]^n$, while executable binaries span a very sparse subset of
the binary space $\{0, 1\}^n$. Similarly, a set of executable source
codes in a given programming language is a sparse subset of all
character strings. Despite the theoretical difficulties several papers
address the issue. \cite{adversarial_malware} propose an obfuscate that optimises
a malicious source code by applying some of the predefined
modifications. The obfuscate method utilises the classifier’s gradient
to choose the most appropriate code modification. The set of plausible
modifications is given beforehand and allows only additive changes.
Although this significantly limits the attacker’s action space, the
authors claim reaching misclassification rates of up to 69\%.
\cite{adversarial_malware_binary} focus on static portable executables which they encode
into a binary feature indicator vectors. Again, additive modifications
are allowed only and malware is optimised with a bit gradient ascent.
\cite{adversarial_malware_pe} take a different approach to malware optimisation and
propose an agent which is trained with reinforcement learning. The agent
is given a portable executable and its goal is to choose the most
suitable modification of a piece of malware to lower the probability of
detection.

\subsection{Game-Theoretical Approach}

As already shown, the problem of adversarial samples can be modelled as
a game of two actors. However, Brückner et al. \cite{stackelberg_games} propose a more general
game model compared to those already mentioned. In particular, the
authors define the players as a classifier and a data generator
consisting of \emph{all} actors generating data – that is the second
player aggregately covers both benign and malicious actors.

This setting is explored using a game-theoretical point of view. The
authors propose a Stackelberg prediction game in which a classifier,
acting as a leader, and a data generator, acting as a follower, optimise
their actions to meet their objectives. They argue the Stackelberg
equilibrium is the most appropriate concept for trainable models,
specifically compared to the Nash equilibrium. It is so, they claim,
mainly because once a model is finalised and deployed, it is not changed
anymore and thus the attacker can potentially learn all details of the
model and adjust its actions to it.

In other words, the actions – the choice of model parameters and the
test time data generation – are not carried out simultaneously, but
instead the classifier commits to a specific parameters vector and the
attacker utilises the information about the model and adjusts its
attacking strategy accordingly. The later is modelled by a distribution
shift at test time. The data generator transforms a probability of data
$p$ to a test time data probability $\dot{p}$ which maximises its
objective function. In addition, the authors show that linear and
kernel-based models together with suitable objective functions allow
reformulating the problem to a quadratic program which yields the
optimal model parameters.

We define our game very similarly to the Stackelberg prediction game. However, we, in contrast, consider the data generated by benign users are stationary; and malicious users only adjust their data in response to the detector. Also, we assume the classifier is complex and largely non-linear, which leads to problems that do not have analytical solutions. And finally, we consider the defender necessarily plays a mixed strategy in order to follow the Stackelberg's equilibrium.

Amin et al. \cite{stackgrad} propose a gradient-based algorithm to solve a normal-form game by identifying a Stackelberg equilibrium. They assume that one player is a defender (playing a leader) and the second player is an attacker (a follower). They model a defender's mixed strategy with a parametrised distribution $D_\theta$ and update its parameters with a gradient descent. To estimate the expectation of the gradient, the authors use a Monte-Carlo method and sample random variables from corresponding distributions – for instance the action played by the defender is sampled from $D_\theta$. With convenient parametrisation of $D_\theta$, this approach provides an algorithm that solves a game in which the action spaces of both players are finite and discrete.

In this work, we follow this approach and similarly estimate the gradient of $D_\theta$ with a Monte-Carlo method. However, in contrast we deal with a game in which the defender's (or in our setting detector's) action space is infinite – it is in fact a family of possible classifiers and the mixed strategy is a probability distribution over all classifiers.

Lockhart et al. \cite{exploitability_descent} propose a general-scheme algorithm that approximately solves an extensive-form game of two players where each is defined by its policy $\pi_i$. The algorithm consists of iterating over a two-step process of both players computing best responses to the policies from the previous step and updating the policies based on the now-generated best responses. They prove that if both players employ this algorithm and update their policies to minimise exploitability, the policies converge to a Nash equilibrium.

In spite of the fact that we aim to find a Stackelberg equilibrium, we take a similar approach and construct an algorithm that in each iteration consists of generating the attacker's best response and updating the defender's mixed strategy $D_\theta$ by minimising its expected risk.

\subsection{Dataset poisoning}

The Stackelberg Prediction Game assumes the model is fixed after
deployment. In practice, however, engineers retrain the model on newly
obtained data that might better represent their population. As this
might be done periodically, the adversary shall take advantage of it and
adjust its obfuscate strategy. Concretely, Rubinstein et al. \cite{antidote}
elaborate on
poisoning anomaly detectors.

The poisoning obfuscate consists of purposely providing pre-crafted
samples to the detector over a long period of time in belief, that the
samples will create a blind spot in which all samples are considered
benign by the detector. The authors assume that the input space is
usually governed by a distribution of benign samples concentrated only
in certain areas, leaving the rest for anomalous activity. Given a
substantial amount of time, the adversary is gradually able to poison
the detector by targeting the large empty parts of the input space and
populating them with benign samples. In future retraining, the anomaly
detector may mistakenly consider those re-populated areas a new
phenomenon and label them benign. The attacker then simply crafts an
obfuscate near to the poisoned areas of the input space.

The authors present that such an obfuscate is possible with an anomaly
detector based on principal component analysis (PCA) which determines
directions of the sample space with greatest variance. Replacing
variance in PCA with median absolute deviation, which in contrary is a
robust scale estimator, their model is robust to data set poisoning and
successfully performs anomaly detection in backbone networks.

Despite dataset poisoning is an interesting problem, it is here mentioned to demonstrate other problems in adversarial machine learning that are not solved in this work.
